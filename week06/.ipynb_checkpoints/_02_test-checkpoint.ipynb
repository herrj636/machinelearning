{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.externals import joblib\n",
    "from my_measures import BinaryClassificationPerformance\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read raw test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(91000, 14)\n"
     ]
    }
   ],
   "source": [
    "amazon = pd.read_csv('/Users/juandherrera/Google Drive/017_Machine Learning/ml/week06/raw_data_test.csv')\n",
    "print(amazon.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arriba Fire Roasted Mexican Salsa is outstanding.  It has a peppery, smoky, roasted tomato flavor and is excellent for dipping chips, cooking, or as a toping.  It does not have a lot of chucky tomatoes or peppers in it, which I prefer, but has a nice, even consistency.  It is an excellent salsa - in fact it's one of the best I've tried.  I prefer the hot variety, as I love spiciness, but they are all good.\n"
     ]
    }
   ],
   "source": [
    "print(amazon.loc[0, \"Text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  Unnamed: 0.1      Id   ProductId          UserId  \\\n",
      "0       36788        482870  482871  B00060NJT8  A1IL6W1NK05UW9   \n",
      "1       54791        110639  110640  B00438XVGU   A3F4CZ5Z52D6L   \n",
      "2      413125        256540  256541  B003JA5KBW  A16PUD6V3KAZ0Q   \n",
      "3       88049        219783  219784  B002L96ULG   AIXCATW18SQPD   \n",
      "4        3244        179002  179003  B0025UCAII  A3RZRJ1YO2RXOQ   \n",
      "\n",
      "                   ProfileName  HelpfulnessNumerator  HelpfulnessDenominator  \\\n",
      "0            C. Baker \"cbaker\"                     0                       0   \n",
      "1                          Jen                     0                       0   \n",
      "2  B. Caruso \"The Happy Aspie\"                     1                       2   \n",
      "3       P. Goldberg \"perihope\"                     4                       4   \n",
      "4                         tina                     0                       0   \n",
      "\n",
      "   Score        Time                                     Summary  \\\n",
      "0      5  1189209600                             Excellent Salsa   \n",
      "1      5  1285632000  Excellent Substitute for Deployed Military   \n",
      "2      1  1292284800       Doesn't work if you can't drink it...   \n",
      "3      5  1281139200                          Delicious cookies!   \n",
      "4      5  1350086400                                   excellent   \n",
      "\n",
      "                                                Text  helpScore  helpful  \n",
      "0  Arriba Fire Roasted Mexican Salsa is outstandi...        NaN    False  \n",
      "1  I bought two of these 50 packs of Starbucks VI...        NaN    False  \n",
      "2  This sounded like a fantastic opportunity to a...        0.5    False  \n",
      "3  These are light and crispy and tart but sweet....        1.0     True  \n",
      "4  The product met my expectations and was delive...        NaN    False  \n",
      "0.07276923076923077\n"
     ]
    }
   ],
   "source": [
    "print(amazon.head())\n",
    "print(amazon['helpful'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction on natural language data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# vectorizer = CountVectorizer()\n",
    "# corpus = amazon.Text.as_matrix()\n",
    "# X_bag_of_words = vectorizer.fit_transform(corpus)\n",
    "# print(X_bag_of_words.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juandherrera/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/hashing.py:102: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(91000, 131072)\n"
     ]
    }
   ],
   "source": [
    "# vectorize Bag of Words from review text; as sparse matrix\n",
    "# from sklearn.feature_extraction.text import HashingVectorizer\n",
    "# hv = HashingVectorizer(n_features=2 ** 17, non_negative=True)\n",
    "hv = joblib.load('hv.pkl')\n",
    "X_hv = hv.transform(amazon.Text)\n",
    "print(X_hv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html\n",
    "# from sklearn.feature_extraction.text import TfidfTransformer\n",
    "# transformer = TfidfTransformer()\n",
    "transformer = joblib.load('transformer.pkl')\n",
    "X_tfidf = transformer.transform(X_hv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(X_tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Create additional quantitative features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features from Amazon.csv to add to feature set\n",
    "amazon['reviewLen'] = amazon['Text'].str.len()\n",
    "\n",
    "X_quant_features = amazon[[\"Score\", \"reviewLen\"]]\n",
    "print(X_quant_features.head(10))\n",
    "print(type(X_quant_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine all quantitative features into a single sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix, hstack\n",
    "X_quant_features_csr = csr_matrix(X_quant_features)\n",
    "X_combined = hstack([X_tfidf, X_quant_features_csr])\n",
    "X_matrix = csr_matrix(X_combined) # convert to sparse matrix\n",
    "print(X_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create `X`, scaled matrix of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature scaling\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# sc = StandardScaler(with_mean=False)\n",
    "sc = joblib.load('sc.pkl')\n",
    "X = sc.transform(X_matrix)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create `y`, vector of Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = amazon['helpful'].values\n",
    "print(type(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict `y` with the fit on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL: ordinary least squares\n",
    "# from sklearn import linear_model\n",
    "# ols = linear_model.SGDClassifier(loss=\"squared_loss\")\n",
    "# ols.fit(X, y)\n",
    "# joblib.dump(ols, 'ols.pkl') # pickle\n",
    "ols = joblib.load('ols.pkl') # pickle\n",
    "\n",
    "ols_performance = BinaryClassificationPerformance(ols.predict(X), y, 'ols')\n",
    "ols_performance.compute_measures()\n",
    "print(ols_performance.performance_measures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL: SVM, linear\n",
    "# from sklearn import linear_model\n",
    "# svm = linear_model.SGDClassifier()\n",
    "# svm.fit(X, y)\n",
    "# joblib.dump(svm, 'svm.pkl') # pickle\n",
    "svm = joblib.load('svm.pkl') # pickle\n",
    "\n",
    "svm_performance = BinaryClassificationPerformance(svm.predict(X), y, 'svm')\n",
    "svm_performance.compute_measures()\n",
    "print(svm_performance.performance_measures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL: logistic regression\n",
    "# from sklearn import linear_model\n",
    "# lgs = linear_model.SGDClassifier(loss='log', n_iter=50, alpha=0.00001)\n",
    "# lgs.fit(X, y)\n",
    "# joblib.dump(lgs, 'lgs.pkl') # pickle\n",
    "lgs = joblib.load('lgs.pkl')\n",
    "\n",
    "lgs_performance = BinaryClassificationPerformance(lgs.predict(X), y, 'lgs')\n",
    "lgs_performance.compute_measures()\n",
    "print(lgs_performance.performance_measures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL: Naive Bayes\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# nbs = MultinomialNB()\n",
    "# nbs.fit(X, y)\n",
    "# joblib.dump(nbs, 'nbs.pkl') # pickle\n",
    "nbs = joblib.load('nbs.pkl')\n",
    "\n",
    "nbs_performance = BinaryClassificationPerformance(nbs.predict(X), y, 'nbs')\n",
    "nbs_performance.compute_measures()\n",
    "print(nbs_performance.performance_measures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL: Perceptron\n",
    "# from sklearn import linear_model\n",
    "# prc = linear_model.SGDClassifier(loss='perceptron')\n",
    "# prc.fit(X, y)\n",
    "# joblib.dump(prc, 'prc.pkl') # pickle\n",
    "prc = joblib.load('prc.pkl')\n",
    "\n",
    "prc_performance = BinaryClassificationPerformance(prc.predict(X), y, 'prc')\n",
    "prc_performance.compute_measures()\n",
    "print(prc_performance.performance_measures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL: Ridge Regression Classifier\n",
    "# from sklearn import linear_model\n",
    "# rdg = linear_model.RidgeClassifier()\n",
    "# rdg.fit(X, y)\n",
    "# joblib.dump(rdg, 'rdg.pkl') # pickle\n",
    "rdg = joblib.load('rdg.pkl')\n",
    "\n",
    "rdg_performance = BinaryClassificationPerformance(rdg.predict(X), y, 'rdg')\n",
    "rdg_performance.compute_measures()\n",
    "print(rdg_performance.performance_measures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC plot to compare performance of various models and fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fits = [svm_performance, lgs_performance, nbs_performance, rdg_performance, prc_performance, ols_performance]\n",
    "\n",
    "for fit in fits:\n",
    "    plt.plot(fit.performance_measures['FP'] / fit.performance_measures['Neg'], \n",
    "             fit.performance_measures['TP'] / fit.performance_measures['Pos'], 'bo')\n",
    "    plt.text(fit.performance_measures['FP'] / fit.performance_measures['Neg'], \n",
    "             fit.performance_measures['TP'] / fit.performance_measures['Pos'], fit.desc)\n",
    "plt.axis([0, 1, 0, 1])\n",
    "plt.title('ROC plot: test set')\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### looking at reviews based on their classification\n",
    "\n",
    "Let's say we decide that Ridge Regression is the best model for generalization. Let's take a look at some of the reviews and try to make a (subjective) determination of whether it's generalizing well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon.loc[0, \"Text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon.loc[0, \"helpful\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_predictions = rdg.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ridge_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# false positives\n",
    "\n",
    "import random, time\n",
    "\n",
    "for i in range(0, len(ridge_predictions)):\n",
    "    if (ridge_predictions[i] == True):\n",
    "        if (amazon.loc[i, \"helpful\"] == False):\n",
    "            if (random.uniform(0, 1) < 0.05):\n",
    "                print(i)\n",
    "                print(time.strftime('%Y-%m-%d %H:%M:%S', time.gmtime(amazon.loc[i, \"Time\"])))\n",
    "                print(amazon.loc[i, \"Score\"])\n",
    "                print(amazon.loc[i, \"HelpfulnessDenominator\"])\n",
    "                print(amazon.loc[i, \"ProductId\"])\n",
    "                print(amazon.loc[i, \"Text\"])\n",
    "                print('* * * * * * * * * ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
